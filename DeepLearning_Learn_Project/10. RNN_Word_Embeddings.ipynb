{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word Embeddings**\n",
    "\n",
    "This is a supervised learning project.\n",
    "\n",
    "1) The first part contains a simple implementation on how to build a NN with an embedding layer.\n",
    "\n",
    "2) Second part is **word to vector** implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras \n",
    "\n",
    "# CReate a list with 10 possible reviews. First 5 are positive last 5 are negative\n",
    "reviews = ['nice food',\n",
    "        'amazing restaurant',\n",
    "        'too good',\n",
    "        'just loved it!',\n",
    "        'will go again',\n",
    "        'horrible food',\n",
    "        'never go there',\n",
    "        'poor service',\n",
    "        'poor quality',\n",
    "        'needs improvement']\n",
    "\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30 \n",
    "# Convert the review into numeric values using one hot encoding \n",
    "encoded_reviews = [one_hot(d, vocab_size) for d in reviews]\n",
    "encoded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to padd the reviess so every sample have equal size\n",
    "max_length = 4\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the embeded vector size\n",
    "embeded_vector_size  = 5\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_reviews\n",
    "y = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y, epochs=50, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f'loss = {loss} and Accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Through the NN training we also automatically get the embedding for this model which are : \n",
    "\n",
    "# Weigths : \n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' For nice the weigths : {weights[2]}')\n",
    "print(f' For good the weigths : {weights[8]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general for bigger datasets we expect similar words ( like nice and good) to have similar values for their weights\n",
    "\n",
    "Another apporach would be to train a model and in a latter stage to load the already trained model's embeddings for training a different model or predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Second Part** Word to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are not hand cragted. Instead, they are learnt during NN training.\n",
    "\n",
    "How it workds :\n",
    "1) Take a fake problem : fill in a missing word in a sentence\n",
    "2) Solve it using NN\n",
    "3) You get word embeddings as a side effect\n",
    "\n",
    "2 Methods to implement that, both of them are called fake problem and our aim is to extract the word embeddings : \n",
    "\n",
    "1) CBOW : Continuous Bag Of Words : Given a context words predict target word\n",
    "2) Skip Gram : Given the target predict context words\n",
    "\n",
    "With the all the above our goal is to be able to transform a word to a vector and be capable to use math on theses vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Exploring the Dataset\n",
    "The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas.\n",
    "\n",
    "Link to the Dataset: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running unzip the \"Cell_Phones_and_Accessories_5.json.gz\"\n",
    "df = pd.read_json(\"Cell_Phones_and_Accessories_5.json\", lines=True) # Line = True means that each line is a particular object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194439, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.reviewText[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a word to vector only using the review column of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we are going to Preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim lib gas easy to use utilities in comparison to tensorflow\n",
    "# For example using the simple preprocess function we are going to tokenize the sentence\n",
    "print(gensim.utils.simple_preprocess(df.reviewText[0]))\n",
    "#Everything now is in lowercases and it will keep only the words by erasing \"!, I etc.\". In some parts we are having an issue let's say don't will become don\n",
    "review_text = df.reviewText.apply(gensim.utils.simple_preprocess) # create a new pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window = 10, # Window of a sentence\n",
    "    min_count = 2, # at least 2 words need to be in a sentence in order to be used in the training\n",
    "    workers = 4, # How many CPUs to use for the training \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary with ubique list of words\n",
    "model.build_vocab(review_text, progress_per = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total examples for training = 194439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61507050, 83868975)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#By default the epochs are set to 5\n",
    "print(f' Total examples for training = {model.corpus_count}')\n",
    "model.train(review_text, total_examples = model.corpus_count, epochs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.6868460178375244),\n",
       " ('shabby', 0.6632394790649414),\n",
       " ('horrible', 0.6291980743408203),\n",
       " ('good', 0.5971243977546692),\n",
       " ('funny', 0.5388672947883606),\n",
       " ('okay', 0.538292646408081),\n",
       " ('awful', 0.533338189125061),\n",
       " ('legit', 0.5261659622192383),\n",
       " ('ok', 0.5207666754722595),\n",
       " ('cheap', 0.5106683969497681)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"bad\") # Find the words that are similar to bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51925254"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1 = \"cheap\", w2 = \"inexpensive\") # Print similarities between 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1 = \"great\", w2 = \"great\") # Print similarities between 2 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
