{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Quantization in Deep Learning**\n",
    "\n",
    "It is a process of reducing the model size so that it can run on EDGE(microprocessors etc.) devices\n",
    "\n",
    "Benefits : \n",
    "\n",
    "1) Run ML models efficiently on EDGE devices\n",
    "2) Faster Inference\n",
    "\n",
    "2 Ways to do it in TensorFLow:\n",
    "\n",
    "1) Post Training Quantization : take your model and implement tf.lite convert. This process will automatically reduce the size. If you also apply quantization with tf.lite then the resulting model will be much more smaller\n",
    "\n",
    "2) Quantization aware training : Apply quantize_model(tf_model) and then you train again q_model.fit() (similar to transfer model). The resulting model will be fine tuned and then you can apply again tf lite to convert for EDGE devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes : \n",
    "\n",
    "(1) Train a hand written digits model\n",
    "\n",
    "(2) Export to a disk and check the size of that model\n",
    "\n",
    "(3) Use two techniques for quantization (1) post training quantization (3) quantization aware trainin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale \n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# FLatten\n",
    "\n",
    "X_train_flattened = X_train.reshape(len(X_train), 28*28)\n",
    "X_test_flattened = X_test.reshape(len(X_test), 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model Using flatten layer to avoid calling reshape \n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.build(input_shape=(None, 28, 28))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ml_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) Post Training Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and convert it in tf model\n",
    "model = tf.keras.models.load_model('ml_model.keras')\n",
    "model.export('exported_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnvert the model with lite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('exported_model')  # Use the new exported model path\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "len(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnvert the model with lite and qunatize it \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('exported_model')  # Use the new exported model path\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "len(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above lengths represent the bytes of each model. The results show that the size of the model with quantization has decreased a lot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "with open(\"tflite_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "with open(\"tflite_quant_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2) Quantization aware training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_aware_model.fit(X_train, y_train, epochs=1)\n",
    "q_aware_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_qaware_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tflite_qaware_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_qaware_model.tflite\", 'wb') as f:\n",
    "    f.write(tflite_qaware_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
